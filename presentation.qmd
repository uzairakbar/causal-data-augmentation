---
title: Symmetry as Intervention
subtitle: |
  An Analysis of Causal Effect Estimation using<br>Outcome Invariant Data Augmentation
  <span style="float: right;">[arXiv:2510.25128](https://arxiv.org/abs/2510.25128)</span>
format:
  metropolis-beamer-revealjs:
    theme: style.scss
    embed-resources: true
    controls: true
    controls-layout: bottom-right
    navigation-mode: linear
    html-math-method: mathjax
    include-in-header:
      - file: "macros.jax"
header-logo: https://neurips.cc/static/core/img/neurips-navbar-logo.svg
header-logo-link: "https://posit.co/"
slide-number: "c"
author:
  - name: Uzair Akbar
    affiliations: Georgia Tech
    orcid: 0009-0001-7826-3578
  - name: Niki Kilbertus
    affiliations: TU Munich
    orcid: 0000-0001-8718-4305
  - name: Hao Shen
    affiliations: TU Munich
    orcid: 0000-0003-0091-4155
  - name: Krikamol Muandet
    affiliations: CISPA
    orcid: 0000-0002-4182-5282
  - name: Bo Dai
    affiliations: Georgia Tech
    orcid: 0009-0002-8070-574X
date: last-modified
bibliography: references.bib
embed-resources: true
---



# Motivation

## Correlation vs. causation


![[xkcd.com/925](https://xkcd.com/925/) by Randall Munroe.](https://imgs.xkcd.com/comics/cell_phones.png){style="image-rendering: auto;"}

::: {.incremental}
- Can we recover causal effects from *observational data*?
- **Yes—but only with untestable assumptions and domain knowledge!**
:::

## This work

<br><br><br>We try to answer the fundamental question:<br>

> Can knowledge of **symmetries in data generation**—often used implicitly in certain regularizers—be repurposed to<br>**improve causal effect estimation** given<br>only observational $(X, Y)$ data?

# Statistical vs. Causal Estimation

## Empirical risk minimization (ERM)
::: {.columns}
::: {.column width="80%"}
For treatment $X$, outcome $Y$ samples generated as
$$
Y = \rfunc{X} + \xi ,
\qquad
\E{\xi} = 0 ,
$$

::: {.fragment fragment-index="1"}
statistical inference entails recovering the optimal predictor $\E{ Y \mid X = \vx }$ [by minimizing the risk]{.fragment fragment-index="2"}
:::

::: {.fragment fragment-index="2"}
$$
R_{\ERM}( \bh ) := \E{ \sqNorm{ Y - \bhyp{X} } }  ,
$$
over hypotheses $\bh\in\H$ from a rich enough class $\H$.
:::

::: {.fragment}
For un-correlated $X$ and noise $\xi$, the minimizer $\bh_{\ERM}{\blue{(}} \vx {\blue{)}}$<br>coincides with the true *causal effect* $\rfunc{\vx}$.
:::
:::
::: {.column width="20%" .rightcol}
![](images/data-generation.svg){width=100% style="float:right;"}
:::
:::






## Data augmentation

::: {.columns}
::: {.column width="80%"}

::: {.fragment fragment-index="1"}
For finite $n$ samples $\D := \{ (\vx_i, \vy_i) \}_{i=0}^n$, regularization<br>techniques are used to mitigate *estimation variance*.
:::

::: {.fragment fragment-index="2"}
<br>E.g., *data augmentation (DA)* achieves this via multiple random augmentations $(\gG \vx_i, \vy_i)$ per sample in the risk
:::

::: {.fragment fragment-index="3"}
<br>$$
R_{\DA+\ERM}( \bh ) := \E{ \sqNorm{ Y - \bhyp{ \gG X } } } ,
\qquad
\gG\sim \P_{ \gG } .
$$
:::
:::

::: {.column width="20%" .rightcol}

![](images/data-generation.svg){width=100% style="float:right;"}

::: {.fragment fragment-index="2"}
![](images/data-generation.svg){width=100% style="float:right;"}
:::

:::
:::







## Confounding bias and spurious correlation
::: {.columns}
::: {.column width="80%"}

::: {.fragment}
[Problem with ERM:]{.alert} Generally, $X$ and $\xi$ are correlated.
:::

::: {.fragment}
This makes the ERM minimizer a biased estimator of $\rf$:
$$
\begin{align*}
Y &= \rfunc{X} + \xi ,\\
\nonumber
\Rightarrow \underbrace{\E{Y \mid X = \vx} }_{\text{ERM minimizer}} &= \rfunc{\vx} + \underbrace{\E{ \xi \mid X = \vx}}_{\text{confounding bias $\neq 0$}} .
\end{align*}
$$
:::

::: {.fragment}
This *spurious correlation* b/w $X$ and $\xi$ arises due to their unobserved common parents, called *confounders*.
:::
:::

::: {.column width="20%" .rightcol}
![](images/confounded-data-generation.svg){width=100% style="float:right;"}
:::
:::



## Intervention for causal estimation
::: {.columns}
::: {.column width="80%"}

::: {.fragment fragment-index="1"}
Removing correlation b/w $X$, $\xi$, requires an *intervention*[—explicitly assigning $X$ some independently sampled $\Xtilde$ during data generation, a.k.a. a *randomized control trial*:
$$
Y = \rfunc{\Xtilde} + \xi
$$]{.fragment fragment-index="2"}
:::

::: {.fragment fragment-index="3"}
Now, doing ERM on samples of $(Y, \Xtilde)$ recovers $\rf$.
:::

::: {.fragment fragment-index="4"}
[Problem:]{.alert} Often not possible to intervene on real systems.<br> We only have access to pre-collected *observational data*.
:::
:::

::: {.column width="20%" .rightcol}

![](images/confounded-data-generation.svg){width=100% style="float:right;"}

::: {.fragment fragment-index="2"}
![](images/intervention.svg){width=100% style="float:right;"}
:::

:::
:::




## Instrumental variables (IVs)
::: {.columns}
::: {.column width="100%"}
![](images/instrument.svg){width=18.4% style="float:right;"}

::: {.fragment}
To workaround interventions, use *instrument* $\gZ$ satisfying
:::

::: {.incremental}
1. treatment relevance $\gZ \nindep X$,
2. exclusion $\gZ\indep Y \mid X$,
3. un-confoundedness $\gZ \indep \xi$,
4. outcome relevance $Y \nindep \gZ$,
:::

::: {.fragment}
Conditioning the model on $\gZ$ gives $\E{ Y \mid \gZ } = \E{ \rfunc{X} \mid \gZ }$, [which can then be solved for $\rf$ by minimizing the risk
$$
R_{\IV}( \bh ) := \E{ \sqNorm{ Y - \E{ \bhyp{X} \mid \gZ } } } .
$$]{.fragment}
:::

::: {.fragment}
[Problem:]{.alert} Instruments are scarce in most application domains.
:::

:::

:::










# Causal Estimation with Data Augmentaiton

## Data augmentation = model symmetries

::: {.fragment}
We restrict ourselves to DA transformations with respect to which $\rf$ is invariant. [Specifically, $\gG$ takes values in $\G$ such that $\rf$ is *$\G$-invariant*:
$$
\rfunc{\vx} = \rfunc{\vg \vx},
\qquad
\forall
\;\;
(\vx, \vg)\in \X\times\G .
$$]{.fragment}
:::

::: {.fragment}
Of course, constructing such DA requires knowledge of symmetries of $\rf$. [E.g., when classifying images $\vx$ of cats vs. dogs, the true labeling function would certainly be invariant to random image rotations $\gG\vx$.
![](images/catRotation.svg){width=90% style="float:center;"} ]{.fragment}
:::





## Data augmentation = soft intervention
::: {.row}
::: {.columns}
::: {.column width="60%"}
::: {.fragment fragment-index="1"}
**Key insight:** When $\rf$ is $\G$-invariant, $(Y, \gG X)$ follows the data generation: 
$$
Y = \rfunc{ \gG X } + \xi .
$$
:::
::: {.fragment fragment-index="2"}
Therefore, DA is equivalent to a soft intervention on the treatment $X$.
:::
:::
::: {.column width="40%" .rightcol}
::: {.fragment fragment-index="2"}
![](images/augmentation-intervention.svg){width=100% style="float:right;"}
:::
:::
:::
:::

::: {.row}
::: {.fragment fragment-index="3"}
$\Rightarrow$ DA+ERM *dominates* vanilla ERM on causal estimation error (CER):
:::
<div class="fragment fade-in" data-fragment-index="3" style="position: absolute;">
<div class="fragment fade-out" data-fragment-index="4" style="position: absolute;">
$$
\CER(\bh) := \E{ \sqNorm{ \rfunc{X} - \bhyp{X} } } , 
\phantom{\qquad
\boxed{
  \CER(\bh_{\DA+\ERM}) \leq \CER(\bh_{\ERM})
} .}
$$
</div>
</div>

::: {.fragment fragment-index="4"}
$$
\CER(\bh) := \E{ \sqNorm{ \rfunc{X} - \bhyp{X} } } , 
\qquad
\boxed{
  \CER(\bh_{\DA+\ERM}) \leq \CER(\bh_{\ERM}) .
}
$$
:::

::: {.incremental  fragment-index="5"}
- Strictly better when DA perturbes spurious features correlated with $\xi$.
- But otherwise performs no worse than ERM.
:::
:::




## Data augmentation = relaxed IVs
::: {.row}
::: {.columns}
::: {.column width="60%"}

::: {.fragment fragment-index="1"}
**Key insight:** DA params $\gG$ are *IV-like (IVL)*[—having IV properties (i)—(iii) by design.]{.fragment fragment-index="2"}
:::

::: {.fragment fragment-index="3"}
<br>Such a relaxation renders IV regression ill-posed, [so we suggest IVL regression:]{.fragment fragment-index="4"}
:::

:::
::: {.column width="40%" .rightcol}
![](images/augmentation-intervention.svg){width=100% style="float:right;"}
:::
:::
:::
::: {.row}
::: {.fragment fragment-index="4"}
$$
R_{\IVL}(\bh) := R_{\IV}(\bh) + \boxed{\alpha \cdot R_{\ERM}(\bh)} \Big\} {\tiny\text{ERM regularizer for ill-posed IV reg.}}
$$
:::

::: {.fragment fragment-index="5"}
$\Rightarrow$ The composition DA+IVL simulates a worst-case/adversarial DA.
:::

::: {.fragment fragment-index="6"}
$\Rightarrow$ DA+IVL *dominates* DA+ERM; better iff spurious features perturbed.
$$
\boxed{\CER(\bh_{\DA+\IVL}) \leq \CER(\bh_{\DA+\ERM}).}
$$
:::

:::















## Data augmentation = causal regularization

::: {.fragment}
**Causal regularization:** Methods that impove causal estimation despite full identification of $\rf$ not being possible.
:::

::: {.fragment}
Why bother?
:::

::: {.incremental}
- **No-regret improvement:** Under our symmetry based DA construction, DA dominates on causal estimation = sometimes better, never worse.
- **Robust prediction:** Reducing confounding bias is estimation reduces sensitivity to spurious features, allowing for more robust predictors under distribution shifts.
:::




# Experiments

## Simulation ablations

::: {.fragment fragment-index="5"}
![](https://uzairakbar.github.io/causal-data-augmentation/sweep_plots.svg){width=100%}
:::

::: {.fragment fragment-index="1"}
Simulation experiment with a linear, centered Gaussian model [with $\rbf\in\R^m$,
confounding strength $\kappa > 0$, and DA strength $\gamma > 0$, s.t.]{.fragment fragment-index="2"}
[<br>$\qquad\qquad\qquad \gG X := X + \gamma\cdot \gG\qquad$  $\gG\in\operatorname{null}(\rbf)$.]{.fragment fragment-index="3"}
[<br>Normalized CER (nCER) $=0$  for true $\rf$ and $1$ for pure confounding.]{.fragment fragment-index="4"}
:::



## Baseline comparison

::: {.fragment fragment-index="2"}
![](https://uzairakbar.github.io/causal-data-augmentation/box_plots.svg){width=100%}
:::

::: {.fragment fragment-index="1"}
Comparison with select causal regularization methods and common domain generalisation baselines. All methods are provided only $(X, Y)$ data along with DA transformations $\gG$—Gausian noise in optical device, and hue, saturation, contrast, translation perturbations, in colored-MNIST.
:::

## Conclusion

![](images/table.svg){width=80%}



## References

::: {#refs}
:::

